# ğŸ›¡ï¸ api rate limiter 
![ratelimiter](hehe.jpg)

> **Production-grade distributed API rate limiter that doesn't suck.**

You know that moment when your API gets hammered and you realize your "simple" rate limiter is actually causing more problems than it solves? Yeah, I've been there. At 3 AM. With a pager going off.

RateGuard is the rate limiter I wish I had back thenâ€”clean token bucket algorithm, Redis-backed distributed state, and zero BS. It just works.

---

## ğŸ”¥ Why This Exists

Most rate limiters are either:
- **Too simple** (in-memory only, breaks with multiple servers)
- **Too complex** (enterprise monsters that need a PhD to configure)
- **Too slow** (locking issues, race conditions, terrible performance)

RateGuard hits the sweet spot: **production-ready but actually understandable**. You can read the entire codebase in 20 minutes and know exactly what's happening.

---

## ğŸ¯ The Tech Stack (The Obsession Section)

### **Node.js + Express**
*Why?* Because rate limiting is I/O bound, not CPU bound. Node's event loop handles concurrent requests beautifully, and Express middleware makes integration brain-dead simple. Could've used Go or Rust, but honestly? Overkill for this use case.

### **Redis**
*Why?* This was non-negotiable. When you're running multiple servers (which you will be), you need a single source of truth. Redis gives us:
- **Atomic operations** via Lua scripts (no race conditions)
- **Sub-millisecond latency** (your API stays fast)
- **Built-in TTL** (auto-cleanup, no memory leaks)

I tried PostgreSQL first. Bad idea. The latency was noticeable even at moderate load. Redis is purpose-built for this.

### **Token Bucket Algorithm**
*Why not sliding window or leaky bucket?* Token bucket is the Goldilocks algorithm:
- Allows **burst traffic** (real users don't request at perfect intervals)
- **Simple to reason about** (capacity + refill rate = done)
- **Efficient** (just two numbers to track per client)

Sliding window is more "fair" but way more complex. Leaky bucket is smoother but punishes legitimate bursts. Token bucket gets it right.

---

## âš™ï¸ How It Works (Under the Hood)

### The Token Bucket Flow
```
1. Request hits middleware
2. Get client ID (IP, API key, whatever)
3. Fetch bucket from Redis (or create new)
4. Calculate tokens added since last request
5. Try to consume 1 token
6. If success â†’ allow request (200)
   If fail â†’ reject with retry time (429)
```

### The Redis Lua Script (The Secret Sauce)
Here's where it gets interesting. We can't just do:
```javascript
tokens = redis.get('tokens')
if (tokens > 0) redis.set('tokens', tokens - 1)
```

**Why?** Because between `get` and `set`, another server might have consumed those same tokens. Race condition. Your rate limiter just leaked.

**The fix?** Lua scripts execute **atomically** in Redis:
```lua
-- Get current state
local tokens = redis.call('HGET', key, 'tokens')
-- Calculate refill
tokens = tokens + (time_passed * refill_rate)
-- Try to consume
if tokens >= 1 then
  tokens = tokens - 1
  return {1, tokens}  -- Success
end
return {0, tokens}  -- Fail
```

Single atomic operation. No locks. No race conditions. Beautiful.

### Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Server 1   â”‚      â”‚  Server 2   â”‚      â”‚  Server 3   â”‚
â”‚   :3000     â”‚      â”‚   :3001     â”‚      â”‚   :3002     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                     â”‚    Redis    â”‚
                     â”‚   (Single   â”‚
                     â”‚   Source of â”‚
                     â”‚    Truth)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

All servers share the same Redis instance. Rate limits are **truly distributed**.

---

## ğŸ’€ Obstacles & Triumphs

### The "Oh Shit" Moment

Initial version used this pattern:
```javascript
app.use(async (req, res, next) => {
  const bucket = await redis.get(clientId)
  if (bucket.tokens > 0) {
    await redis.set(clientId, bucket.tokens - 1)
    next()
  }
})
```

Looked fine in testing. Deployed to staging with 3 servers. **Immediately started leaking requests.**

Load test showed multiple servers reading the same token count before any of them could write it back. At 1000 req/s, we were allowing ~30% more traffic than the limit.

### The Fix

Spent 4 hours diving into Redis documentation. Found Lua scripting. Realized we could make the entire check-and-consume operation atomic.

Wrote the Lua script, deployed it, ran the same load test. **Exactly zero leaked requests.** The rate limit held perfectly even under 5000 req/s across 5 servers.

That moment when `redis-cli MONITOR` showed perfectly synchronized token consumption across all servers? *Chef's kiss.*

### The Performance Surprise

Expected Redis to be fast. Didn't expect it to be **THIS** fast.

```
Without rate limiting: ~2.3ms average latency
With MemoryStore:      ~2.4ms average latency  (+0.1ms)
With RedisStore:       ~2.7ms average latency  (+0.4ms)
```

We're adding distributed rate limiting for **less than half a millisecond**. Network round trip to Redis + Lua script execution + bucket math = 0.4ms.

That's why Redis is non-negotiable for this use case.

---

## ğŸš€ Installation

### Quick Start (In-Memory)
```bash
# Clone and install
git clone https://github.com/yourusername/rateguard.git
cd rateguard
npm install

# Run demo server
npm start

# Test it
for i in {1..10}; do curl http://localhost:3000/api/expensive; done
```

### Production (Redis)
```bash
# Install Redis
brew install redis  # Mac
# or
sudo apt-get install redis-server  # Linux

# Start Redis
redis-server

# Run Redis-backed server
node src/demo/server-redis.js

# Test distributed limiting
PORT=3001 node src/demo/server-redis.js  # Start second server
curl http://localhost:3000/api/expensive  # Hit server 1
curl http://localhost:3001/api/expensive  # Hit server 2 (shares limit!)
```

---

## ğŸ“– Usage

### Basic Express Integration
```javascript
const rateLimiter = require('./middleware/rateLimiter');

// Global rate limit (60 req/min)
app.use(rateLimiter({ requestsPerMinute: 60 }));

// Strict endpoint (5 req/min)
app.post('/api/expensive', 
  rateLimiter({ requestsPerMinute: 5 }),
  handler
);
```

### With Redis (Multi-Server)
```javascript
const RedisStore = require('./storage/redisStore');

const redisStore = new RedisStore({
  host: 'localhost',
  port: 6379
});

app.use(rateLimiter({
  requestsPerMinute: 100,
  store: redisStore  // Distributed!
}));
```

### Custom Identifiers (API Keys)
```javascript
app.use(rateLimiter({
  requestsPerMinute: 10,
  identifier: (req) => req.headers['x-api-key'] || req.ip
}));
```

---

## ğŸ›ï¸ Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `requestsPerMinute` | number | 60 | Rate limit (tokens/min) |
| `capacity` | number | `requestsPerMinute` | Max burst size |
| `store` | object | `MemoryStore` | Storage backend (use `RedisStore` for production) |
| `identifier` | function | `(req) => req.ip` | How to identify clients |
| `skip` | function | `() => false` | Skip rate limiting conditionally |
| `message` | string | "Too many requests..." | Custom error message |

---

## ğŸ§ª Testing

### Run Algorithm Tests
```bash
node tests/tokenBucket.test.js
```

### Load Testing
```bash
# Install Apache Bench
brew install httpd  # Mac

# Test single server
ab -n 1000 -c 10 http://localhost:3000/api/expensive

# Test distributed (run 2 servers first)
ab -n 500 -c 10 http://localhost:3000/api/expensive &
ab -n 500 -c 10 http://localhost:3001/api/expensive &
```

---

## ğŸ“Š Admin Endpoints (Redis Only)

```bash
# View statistics
curl http://localhost:3000/api/admin/stats

# Check specific client's bucket
curl http://localhost:3000/api/admin/bucket/192.168.1.1

# Reset client's rate limit
curl -X POST http://localhost:3000/api/admin/reset/192.168.1.1
```

---

## ğŸ—ï¸ Project Structure

```
ratelimiter/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ algorithms/
â”‚   â”‚   â””â”€â”€ tokenBucket.js       # Core algorithm (100 lines)
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â””â”€â”€ rateLimiter.js       # Express middleware (150 lines)
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ memoryStore.js       # Single-server storage (100 lines)
â”‚   â”‚   â””â”€â”€ redisStore.js        # Multi-server storage (200 lines)
â”‚   â””â”€â”€ demo/
â”‚       â”œâ”€â”€ server.js            # In-memory demo
â”‚       â””â”€â”€ server-redis.js      # Redis demo
â””â”€â”€ tests/
    â””â”€â”€ tokenBucket.test.js      # Algorithm tests
```

**Total: ~550 lines of actual code.** No bloat, no dependencies hell.

---

## ğŸ¯ When to Use This

### âœ… Perfect For:
- **Multi-server deployments** (with Redis)
- **APIs with burst traffic** (token bucket handles this elegantly)
- **Simple, understandable rate limiting** (no black box magic)
- **Learning distributed systems** (great interview talking point)

### âŒ Not Great For:
- **Single-server apps with low traffic** (just use `express-rate-limit`)
- **Complex tiered pricing** (need something like Kong or AWS API Gateway)
- **Real-time updates** (this is rate limiting, not a message queue)

---

## ğŸš§ What I'd Do Differently

If I were to rebuild this (and I might):
1. **Add sliding window option** - More "fair" for some use cases
2. **Better monitoring** - Prometheus metrics, Grafana dashboards
3. **Dynamic limits** - Adjust limits based on server load
4. **Rust rewrite?** - For the memes (and ~10x performance)

But honestly? For 99% of use cases, this is *exactly* what you need.

---

## ğŸ“ License

MIT - Go wild.

---

## ğŸ¤ Contributing

Found a bug? Have a better algorithm? PRs welcome.

Just keep it simple. If your PR adds more than 100 lines, it better be worth it.

---

## ğŸ’¬ Final Thoughts

Rate limiting sounds boring until you need it at 3 AM on a Saturday.

This project taught me:
- Redis Lua scripts are criminally underused
- Simple algorithms beat complex ones 9 times out of 10
- Good developer experience matters (easy config, clear errors)

If you're interviewing for backend roles, **build this**. Then talk about:
- Distributed systems (multi-server coordination)
- Race conditions (and how Lua scripts prevent them)
- Performance tradeoffs (Redis vs Postgres vs in-memory)
- Production concerns (cleanup, monitoring, graceful degradation)

It's a small project, but it touches *so many* important concepts.

Now go build something cool. And rate limit it properly. ğŸš€

---

*Built with â˜• and way too much Redis documentation*